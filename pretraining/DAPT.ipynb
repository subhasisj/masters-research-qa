{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain adaptive Pretraining\n",
    "#### Language: `Arabic`\n",
    "\n",
    "Continual pretraining on the unlablled translated corpus to improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json # to read json\n",
    "import warnings # to ignore warnings\n",
    "from utils import squad_json_to_dataframe_train\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "language = 'vi'\n",
    "input_file_path = f'../SQuAD/translate-train/squad.translate.train.en-{language}.json'\n",
    "record_path = ['data','paragraphs','qas','answers']\n",
    "train = squad_json_to_dataframe_train(input_file_path=input_file_path,record_path=record_path,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5726e1755951b619008f8141</td>\n",
       "      <td>Tên của nhà vật lý người Hà Lan đã phát minh r...</td>\n",
       "      <td>Vào tháng 10 năm 1745, Ewald Georg von Kleist ...</td>\n",
       "      <td>549</td>\n",
       "      <td>Pieter van Musschenbroek</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>572fde9204bcaa1900d76e05</td>\n",
       "      <td>Môn thể thao Olympic nào mà Hrant Shahinyan th...</td>\n",
       "      <td>Trước năm 1992, người Armenia sẽ tham gia Thế ...</td>\n",
       "      <td>430</td>\n",
       "      <td>thể dục dụng cụ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56d37d7f59d6e4140014651a</td>\n",
       "      <td>Vòng loại nào Chris Medina đã bị loại trong mù...</td>\n",
       "      <td>Một trong những thí sinh nổi bật hơn trong năm...</td>\n",
       "      <td>191</td>\n",
       "      <td>Top 40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57318dc8a5e9cc1400cdc061</td>\n",
       "      <td>Spielberg đã ký bao nhiêu bộ phim truyền hình?</td>\n",
       "      <td>Dựa trên sức mạnh công việc của mình, Universa...</td>\n",
       "      <td>77</td>\n",
       "      <td>bốn</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57331b0dd058e614000b56fd</td>\n",
       "      <td>Lý thuyết chủ yếu của thực tế mà Whitehead phả...</td>\n",
       "      <td>Bắt đầu từ cuối những năm 1910 và đầu những nă...</td>\n",
       "      <td>469</td>\n",
       "      <td>thực tế được xây dựng cơ bản bởi các bit vật c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      index  \\\n",
       "0  5726e1755951b619008f8141   \n",
       "1  572fde9204bcaa1900d76e05   \n",
       "2  56d37d7f59d6e4140014651a   \n",
       "3  57318dc8a5e9cc1400cdc061   \n",
       "4  57331b0dd058e614000b56fd   \n",
       "\n",
       "                                            question  \\\n",
       "0  Tên của nhà vật lý người Hà Lan đã phát minh r...   \n",
       "1  Môn thể thao Olympic nào mà Hrant Shahinyan th...   \n",
       "2  Vòng loại nào Chris Medina đã bị loại trong mù...   \n",
       "3     Spielberg đã ký bao nhiêu bộ phim truyền hình?   \n",
       "4  Lý thuyết chủ yếu của thực tế mà Whitehead phả...   \n",
       "\n",
       "                                             context  answer_start  \\\n",
       "0  Vào tháng 10 năm 1745, Ewald Georg von Kleist ...           549   \n",
       "1  Trước năm 1992, người Armenia sẽ tham gia Thế ...           430   \n",
       "2  Một trong những thí sinh nổi bật hơn trong năm...           191   \n",
       "3  Dựa trên sức mạnh công việc của mình, Universa...            77   \n",
       "4  Bắt đầu từ cuối những năm 1910 và đầu những nă...           469   \n",
       "\n",
       "                                                text  c_id  \n",
       "0                           Pieter van Musschenbroek     0  \n",
       "1                                    thể dục dụng cụ     1  \n",
       "2                                             Top 40     2  \n",
       "3                                                bốn     3  \n",
       "4  thực tế được xây dựng cơ bản bởi các bit vật c...     4  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tên của nhà vật lý người Hà Lan đã phát minh r...</td>\n",
       "      <td>Vào tháng 10 năm 1745, Ewald Georg von Kleist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Môn thể thao Olympic nào mà Hrant Shahinyan th...</td>\n",
       "      <td>Trước năm 1992, người Armenia sẽ tham gia Thế ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vòng loại nào Chris Medina đã bị loại trong mù...</td>\n",
       "      <td>Một trong những thí sinh nổi bật hơn trong năm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spielberg đã ký bao nhiêu bộ phim truyền hình?</td>\n",
       "      <td>Dựa trên sức mạnh công việc của mình, Universa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lý thuyết chủ yếu của thực tế mà Whitehead phả...</td>\n",
       "      <td>Bắt đầu từ cuối những năm 1910 và đầu những nă...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Tên của nhà vật lý người Hà Lan đã phát minh r...   \n",
       "1  Môn thể thao Olympic nào mà Hrant Shahinyan th...   \n",
       "2  Vòng loại nào Chris Medina đã bị loại trong mù...   \n",
       "3     Spielberg đã ký bao nhiêu bộ phim truyền hình?   \n",
       "4  Lý thuyết chủ yếu của thực tế mà Whitehead phả...   \n",
       "\n",
       "                                             context  \n",
       "0  Vào tháng 10 năm 1745, Ewald Georg von Kleist ...  \n",
       "1  Trước năm 1992, người Armenia sẽ tham gia Thế ...  \n",
       "2  Một trong những thí sinh nổi bật hơn trong năm...  \n",
       "3  Dựa trên sức mạnh công việc của mình, Universa...  \n",
       "4  Bắt đầu từ cuối những năm 1910 và đầu những nă...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop([\"index\",\"answer_start\",\"text\",\"c_id\"],axis = 1,inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_data_path = f\"./squad.pretraining.{language}.csv\"\n",
    "train.to_csv(pretraining_data_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LineByLineTextDataset\n",
    "\n",
    "# model_name = \"microsoft/Multilingual-MiniLM-L12-H384\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=pretraining_data_path, block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForMaskedLM\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True, mlm_probability=0.15)    \n",
    "\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Ar-Mulitlingula-MiniLM\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=8,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/resolve/main/config.json from cache at C:\\Users\\Subha/.cache\\huggingface\\transformers\\12a5ad52cb7fc5542e16e354fe6eb487f2f87edac63bf85dc238b1236dbaf24c.ccf88548169a21266c411bcf65585ba761d762a9c85fde572f529806fdd94ee2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/Multilingual-MiniLM-L12-H384\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"XLMRobertaTokenizer\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/resolve/main/pytorch_model.bin from cache at C:\\Users\\Subha/.cache\\huggingface\\transformers\\e1243df19ff0e1975c063fc4c531ba7fdad1ee538e0d94c41a766febfee0c8ab.3d27c3b243133a56a858d62deffdc59141c45422837cf3fde167b873bad92273\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at microsoft/Multilingual-MiniLM-L12-H384 and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/Multilingual-MiniLM-L12-H384\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 86788\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10849\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 6.00 GiB total capacity; 4.39 GiB already allocated; 0 bytes free; 4.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Repositories\\masters-research-qa\\pretraining\\DAPT.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositories/masters-research-qa/pretraining/DAPT.ipynb#ch0000008?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositories/masters-research-qa/pretraining/DAPT.ipynb#ch0000008?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositories/masters-research-qa/pretraining/DAPT.ipynb#ch0000008?line=2'>3</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositories/masters-research-qa/pretraining/DAPT.ipynb#ch0000008?line=3'>4</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositories/masters-research-qa/pretraining/DAPT.ipynb#ch0000008?line=4'>5</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mdataset)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repositories/masters-research-qa/pretraining/DAPT.ipynb#ch0000008?line=6'>7</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\transformers\\trainer.py:1422\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1419'>1420</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1420'>1421</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1421'>1422</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1423'>1424</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1424'>1425</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1425'>1426</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1426'>1427</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1427'>1428</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1428'>1429</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=1429'>1430</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\transformers\\trainer.py:2011\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2007'>2008</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2009'>2010</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2010'>2011</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2012'>2013</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2013'>2014</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\transformers\\trainer.py:2043\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2040'>2041</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2041'>2042</a>\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2042'>2043</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2043'>2044</a>\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2044'>2045</a>\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2045'>2046</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1358\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1342'>1343</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1343'>1344</a>\u001b[0m     input_ids,\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1344'>1345</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1353'>1354</a>\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1354'>1355</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1356'>1357</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1357'>1358</a>\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls(sequence_output)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1359'>1360</a>\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1360'>1361</a>\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:685\u001b[0m, in \u001b[0;36mBertOnlyMLMHead.forward\u001b[1;34m(self, sequence_output)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=683'>684</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, sequence_output: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=684'>685</a>\u001b[0m     prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictions(sequence_output)\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=685'>686</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m prediction_scores\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:675\u001b[0m, in \u001b[0;36mBertLMPredictionHead.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=672'>673</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=673'>674</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(hidden_states)\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=674'>675</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(hidden_states)\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/models/bert/modeling_bert.py?line=675'>676</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/torch/nn/functional.py?line=1847'>1848</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 6.00 GiB total capacity; 4.39 GiB already allocated; 0 bytes free; 4.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "988aea983bc49b4de6555a9151e99301026502c3eb4adf0dc68b340a087613cc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('all-purpose-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
