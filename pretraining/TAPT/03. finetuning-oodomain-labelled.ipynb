{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5oS5LUAoEu_",
        "outputId": "9f09bc6c-0f7d-4a7b-e22d-e99b38e06026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri May 13 16:48:17 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 512.15       Driver Version: 512.15       CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   58C    P8     8W /  N/A |      0MiB /  6144MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h9Ts9sVGoDwC"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import json # to read json\n",
        "from datasets import load_dataset,Dataset,get_dataset_config_names,DatasetDict\n",
        "import warnings # to ignore warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "ba936bb9738a49d0b2b89f6e7724525a",
            "8d7e8ce3bc15458bae47fba843b3b75b",
            "15f6efce0301452899e29953d79f51ab",
            "232b82c2311b488e891e5a47490a986a",
            "d203ced429e540ad9325d8887fccfbaf",
            "cee7e824a6864589bc9d58e0bb1a2397",
            "cbac3350f96a4fdebb45b71a9bec1510",
            "d865c31a5b5c46debf61dbd1a3d92227",
            "51fe991ba33d4b7eaa83bf4a86c04a47",
            "37302452a84048ae9fb47ab74952188e",
            "084772ca72054d339b2437fe17ba88ba",
            "56c490a66b2f47d59489974e905ef1f2",
            "66e8b8093a054f9bbe32d8b9b22637d9",
            "020a97d66b384582a0d0e6fbc82d909e",
            "bc8403ee9ec24b8c90334b1e58dfccd5",
            "3a4b4338218b481eb63eeed22d73c809",
            "38502d84bf4d42d5a9d0cf6b3e647f54"
          ]
        },
        "id": "fpVIvGgkoDwF",
        "outputId": "827f7a41-15ff-49c1-850c-47cca044f38a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01232298f2bf4e06a0277ff8828f6479",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZyyzwzuYoDwF"
      },
      "outputs": [],
      "source": [
        "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    verbose: 0 to suppress it default is 1\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(input_file_path).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "    js['q_idx'] = ndx\n",
        "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kuTEOcWBoDwG"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "language = 'vi'\n",
        "input_file_path = f'../../SQuAD/translate-train/squad.translate.train.en-{language}.json'\n",
        "record_path = ['data','paragraphs','qas','answers']\n",
        "train = squad_json_to_dataframe_train(input_file_path=input_file_path,record_path=record_path,verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "uzU8CZaZoDwH",
        "outputId": "2d8dc51f-36b7-4be0-f6cf-0d1869aac292"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5726e1755951b619008f8141</td>\n",
              "      <td>Tên của nhà vật lý người Hà Lan đã phát minh r...</td>\n",
              "      <td>Vào tháng 10 năm 1745, Ewald Georg von Kleist ...</td>\n",
              "      <td>549</td>\n",
              "      <td>Pieter van Musschenbroek</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>572fde9204bcaa1900d76e05</td>\n",
              "      <td>Môn thể thao Olympic nào mà Hrant Shahinyan th...</td>\n",
              "      <td>Trước năm 1992, người Armenia sẽ tham gia Thế ...</td>\n",
              "      <td>430</td>\n",
              "      <td>thể dục dụng cụ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56d37d7f59d6e4140014651a</td>\n",
              "      <td>Vòng loại nào Chris Medina đã bị loại trong mù...</td>\n",
              "      <td>Một trong những thí sinh nổi bật hơn trong năm...</td>\n",
              "      <td>191</td>\n",
              "      <td>Top 40</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57318dc8a5e9cc1400cdc061</td>\n",
              "      <td>Spielberg đã ký bao nhiêu bộ phim truyền hình?</td>\n",
              "      <td>Dựa trên sức mạnh công việc của mình, Universa...</td>\n",
              "      <td>77</td>\n",
              "      <td>bốn</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57331b0dd058e614000b56fd</td>\n",
              "      <td>Lý thuyết chủ yếu của thực tế mà Whitehead phả...</td>\n",
              "      <td>Bắt đầu từ cuối những năm 1910 và đầu những nă...</td>\n",
              "      <td>469</td>\n",
              "      <td>thực tế được xây dựng cơ bản bởi các bit vật c...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  \\\n",
              "0  5726e1755951b619008f8141   \n",
              "1  572fde9204bcaa1900d76e05   \n",
              "2  56d37d7f59d6e4140014651a   \n",
              "3  57318dc8a5e9cc1400cdc061   \n",
              "4  57331b0dd058e614000b56fd   \n",
              "\n",
              "                                            question  \\\n",
              "0  Tên của nhà vật lý người Hà Lan đã phát minh r...   \n",
              "1  Môn thể thao Olympic nào mà Hrant Shahinyan th...   \n",
              "2  Vòng loại nào Chris Medina đã bị loại trong mù...   \n",
              "3     Spielberg đã ký bao nhiêu bộ phim truyền hình?   \n",
              "4  Lý thuyết chủ yếu của thực tế mà Whitehead phả...   \n",
              "\n",
              "                                             context  answer_start  \\\n",
              "0  Vào tháng 10 năm 1745, Ewald Georg von Kleist ...           549   \n",
              "1  Trước năm 1992, người Armenia sẽ tham gia Thế ...           430   \n",
              "2  Một trong những thí sinh nổi bật hơn trong năm...           191   \n",
              "3  Dựa trên sức mạnh công việc của mình, Universa...            77   \n",
              "4  Bắt đầu từ cuối những năm 1910 và đầu những nă...           469   \n",
              "\n",
              "                                                text  c_id  \n",
              "0                           Pieter van Musschenbroek     0  \n",
              "1                                    thể dục dụng cụ     1  \n",
              "2                                             Top 40     2  \n",
              "3                                                bốn     3  \n",
              "4  thực tế được xây dựng cơ bản bởi các bit vật c...     4  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "KrDpmeTlqgap",
        "outputId": "dc4d820b-0f8b-4014-cad5-4eb5e2b58591"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5726e1755951b619008f8141</td>\n",
              "      <td>Tên của nhà vật lý người Hà Lan đã phát minh ra Leyden Jar là gì?</td>\n",
              "      <td>Vào tháng 10 năm 1745, Ewald Georg von Kleist ở Pomerania, Đức, nhận thấy rằng điện tích có thể được lưu trữ bằng cách kết nối một máy phát tĩnh điện cao áp bằng một sợi dây với một thể tích nước trong một lọ thủy tinh cầm tay. Tay và nước của Von Kleist đóng vai trò là chất dẫn điện và bình như một chất điện môi (mặc dù chi tiết về cơ chế được xác định không chính xác tại thời điểm đó). Von Kleist nhận thấy rằng việc chạm vào dây dẫn đến một tia lửa mạnh mẽ, đau đớn hơn nhiều so với việc lấy từ máy tĩnh điện. Năm sau, nhà vật lý người Hà Lan Pieter van Musschenbroek đã phát minh ra một tụ điện tương tự, được đặt tên là bình Leyden, sau Đại học Leiden nơi ông làm việc. Ông cũng bị ấn tượng bởi sức mạnh của cú sốc mà ông nhận được, viết: \"Tôi sẽ không chịu cú sốc thứ hai cho vương quốc Pháp.\"</td>\n",
              "      <td>549</td>\n",
              "      <td>Pieter van Musschenbroek</td>\n",
              "      <td>0</td>\n",
              "      <td>{'answer_start': [549], 'text': ['Pieter van Musschenbroek']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  \\\n",
              "0  5726e1755951b619008f8141   \n",
              "\n",
              "                                                            question  \\\n",
              "0  Tên của nhà vật lý người Hà Lan đã phát minh ra Leyden Jar là gì?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              context  \\\n",
              "0  Vào tháng 10 năm 1745, Ewald Georg von Kleist ở Pomerania, Đức, nhận thấy rằng điện tích có thể được lưu trữ bằng cách kết nối một máy phát tĩnh điện cao áp bằng một sợi dây với một thể tích nước trong một lọ thủy tinh cầm tay. Tay và nước của Von Kleist đóng vai trò là chất dẫn điện và bình như một chất điện môi (mặc dù chi tiết về cơ chế được xác định không chính xác tại thời điểm đó). Von Kleist nhận thấy rằng việc chạm vào dây dẫn đến một tia lửa mạnh mẽ, đau đớn hơn nhiều so với việc lấy từ máy tĩnh điện. Năm sau, nhà vật lý người Hà Lan Pieter van Musschenbroek đã phát minh ra một tụ điện tương tự, được đặt tên là bình Leyden, sau Đại học Leiden nơi ông làm việc. Ông cũng bị ấn tượng bởi sức mạnh của cú sốc mà ông nhận được, viết: \"Tôi sẽ không chịu cú sốc thứ hai cho vương quốc Pháp.\"   \n",
              "\n",
              "   answer_start                      text  c_id  \\\n",
              "0           549  Pieter van Musschenbroek     0   \n",
              "\n",
              "                                                         answers  \n",
              "0  {'answer_start': [549], 'text': ['Pieter van Musschenbroek']}  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_answers(x):\n",
        "    start = x[0]\n",
        "    text = x[1]\n",
        "    return {\n",
        "        'answer_start': [start],\n",
        "        'text': [text]\n",
        "    }\n",
        "\n",
        "train['answers'] = train[['answer_start', 'text']].apply(get_answers, axis=1)\n",
        "pd.set_option('display.max_colwidth',None)\n",
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "m35c0mGloDwI",
        "outputId": "6a9c9c9f-e8fc-4bd3-bffe-1e15e410b87f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5726e179dd62a815002e93b1</td>\n",
              "      <td>Những dược sĩ nào có khả năng tìm kiếm giáo dục bổ sung sau trường dược?</td>\n",
              "      <td>Do sự phức tạp của thuốc bao gồm chỉ định cụ thể, hiệu quả của chế độ điều trị, an toàn của thuốc (nghĩa là tương tác thuốc) và các vấn đề tuân thủ của bệnh nhân (trong bệnh viện và tại nhà), nhiều dược sĩ thực hành tại bệnh viện được giáo dục và đào tạo nhiều hơn sau khi học trường dược một cư dân hành nghề dược và đôi khi theo sau là một cư dân khác trong một khu vực cụ thể. Những dược sĩ đó thường được gọi là dược sĩ lâm sàng và họ thường chuyên về các ngành dược khác nhau. Ví dụ, có những dược sĩ chuyên về huyết học / ung thư, HIV / AIDS, bệnh truyền nhiễm, chăm sóc quan trọng, thuốc cấp cứu, độc dược, dược học hạt nhân, quản lý đau, tâm thần, phòng chống đông máu, thảo dược, quản lý thần kinh / động kinh, nhi khoa , dược sĩ sơ sinh và nhiều hơn nữa.</td>\n",
              "      <td>416</td>\n",
              "      <td>dược sĩ lâm sàng</td>\n",
              "      <td>0</td>\n",
              "      <td>{'answer_start': [416], 'text': ['dược sĩ lâm sàng']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  \\\n",
              "0  5726e179dd62a815002e93b1   \n",
              "\n",
              "                                                                   question  \\\n",
              "0  Những dược sĩ nào có khả năng tìm kiếm giáo dục bổ sung sau trường dược?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        context  \\\n",
              "0  Do sự phức tạp của thuốc bao gồm chỉ định cụ thể, hiệu quả của chế độ điều trị, an toàn của thuốc (nghĩa là tương tác thuốc) và các vấn đề tuân thủ của bệnh nhân (trong bệnh viện và tại nhà), nhiều dược sĩ thực hành tại bệnh viện được giáo dục và đào tạo nhiều hơn sau khi học trường dược một cư dân hành nghề dược và đôi khi theo sau là một cư dân khác trong một khu vực cụ thể. Những dược sĩ đó thường được gọi là dược sĩ lâm sàng và họ thường chuyên về các ngành dược khác nhau. Ví dụ, có những dược sĩ chuyên về huyết học / ung thư, HIV / AIDS, bệnh truyền nhiễm, chăm sóc quan trọng, thuốc cấp cứu, độc dược, dược học hạt nhân, quản lý đau, tâm thần, phòng chống đông máu, thảo dược, quản lý thần kinh / động kinh, nhi khoa , dược sĩ sơ sinh và nhiều hơn nữa.   \n",
              "\n",
              "   answer_start              text  c_id  \\\n",
              "0           416  dược sĩ lâm sàng     0   \n",
              "\n",
              "                                                 answers  \n",
              "0  {'answer_start': [416], 'text': ['dược sĩ lâm sàng']}  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Validation data\n",
        "language = 'vi'\n",
        "input_file_path = f'../../SQuAD/translate-dev/squad.translate.dev.en-{language}.json'\n",
        "record_path = ['data','paragraphs','qas','answers']\n",
        "dev = squad_json_to_dataframe_train(input_file_path=input_file_path,record_path=record_path,verbose=0)\n",
        "dev['answers'] = dev[['answer_start', 'text']].apply(get_answers, axis=1)\n",
        "pd.set_option('display.max_colwidth',None)\n",
        "dev.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UtdMAejjoDwI",
        "outputId": "b0972c68-05ce-483b-caec-8677fd583307"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5726e179dd62a815002e93b1</td>\n",
              "      <td>Những dược sĩ nào có khả năng tìm kiếm giáo dục bổ sung sau trường dược?</td>\n",
              "      <td>Do sự phức tạp của thuốc bao gồm chỉ định cụ thể, hiệu quả của chế độ điều trị, an toàn của thuốc (nghĩa là tương tác thuốc) và các vấn đề tuân thủ của bệnh nhân (trong bệnh viện và tại nhà), nhiều dược sĩ thực hành tại bệnh viện được giáo dục và đào tạo nhiều hơn sau khi học trường dược một cư dân hành nghề dược và đôi khi theo sau là một cư dân khác trong một khu vực cụ thể. Những dược sĩ đó thường được gọi là dược sĩ lâm sàng và họ thường chuyên về các ngành dược khác nhau. Ví dụ, có những dược sĩ chuyên về huyết học / ung thư, HIV / AIDS, bệnh truyền nhiễm, chăm sóc quan trọng, thuốc cấp cứu, độc dược, dược học hạt nhân, quản lý đau, tâm thần, phòng chống đông máu, thảo dược, quản lý thần kinh / động kinh, nhi khoa , dược sĩ sơ sinh và nhiều hơn nữa.</td>\n",
              "      <td>416</td>\n",
              "      <td>dược sĩ lâm sàng</td>\n",
              "      <td>0</td>\n",
              "      <td>{'answer_start': [416], 'text': ['dược sĩ lâm sàng']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5727580bf1498d1400e8f69e</td>\n",
              "      <td>Cách đánh vần nào của Genghis phù hợp nhất với cách phát âm có thể xảy ra của nó?</td>\n",
              "      <td>Một giả thuyết cho rằng cái tên này bắt nguồn từ một phiên bản được khai hóa của từ tiếng Mông Cổ và tiếng Thổ Nhĩ Kỳ, có nghĩa là \"đại dương\", \"đại dương\" hoặc \"trải rộng\". (Hồ Baikal và đại dương được người Mông Cổ gọi là tenggis. Tuy nhiên, dường như nếu họ có ý định gọi Genghis tenggis thì họ có thể đã nói, và viết, \"Tenggis Khan\", mà họ không biết.) Zhèng (tiếng Trung:) có nghĩa là \"đúng\", \"chỉ\" hoặc \"đúng\", sẽ nhận được từ bổ nghĩa tính từ Mông Cổ -s, tạo ra \"Jenggis\", trong tiếng La Mã thời trung cổ sẽ được viết là \"Genghis\". Có khả năng cách phát âm tiếng Mông Cổ của thế kỷ 13 sẽ rất phù hợp với \"Chính tả\".</td>\n",
              "      <td>469</td>\n",
              "      <td>\"Jenggis</td>\n",
              "      <td>1</td>\n",
              "      <td>{'answer_start': [469], 'text': ['\"Jenggis']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>572665ff708984140094c4c3</td>\n",
              "      <td>Nguyên tắc nào liên quan đến sự hình thành các lỗi và tuổi của các chuỗi mà chúng cắt?</td>\n",
              "      <td>Nguyên tắc mối quan hệ xuyên suốt liên quan đến sự hình thành các lỗi và tuổi của các chuỗi mà qua đó chúng cắt. Lỗi là trẻ hơn những tảng đá họ cắt; theo đó, nếu một lỗi được tìm thấy thâm nhập vào một số hình dạng chứ không phải các lỗi trên nó, thì các hình dạng bị cắt cũ hơn lỗi và các phần không bị cắt phải nhỏ hơn lỗi. Tìm giường chính trong những tình huống này có thể giúp xác định xem lỗi là lỗi thông thường hay lỗi lực đẩy.</td>\n",
              "      <td>11</td>\n",
              "      <td>mối quan hệ xuyên suốt</td>\n",
              "      <td>2</td>\n",
              "      <td>{'answer_start': [11], 'text': ['mối quan hệ xuyên suốt']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57308f6b8ab72b1400f9c581</td>\n",
              "      <td>Các văn bản phương Tây đầu tiên đề cập đến phương Đông mô tả người dân là gì?</td>\n",
              "      <td>Chủ nghĩa phương Đông, theo lý thuyết của Edward Said, đề cập đến cách phương Tây phát triển một địa lý tưởng tượng của phương Đông. Địa lý tưởng tượng này dựa trên một diễn ngôn thiết yếu đại diện cho cả sự đa dạng cũng như thực tế xã hội của phương Đông. Thay vào đó, bằng cách thiết yếu phương Đông, bài diễn văn này sử dụng ý tưởng về bản sắc dựa trên địa điểm để tạo ra sự khác biệt và khoảng cách giữa \"chúng ta\" phương Tây và \"họ\" phương Đông, hay \"ở đây\" ở phương Tây và \"ở đó\" ở phương Đông. Sự khác biệt này đặc biệt rõ ràng trong các tác phẩm văn bản và hình ảnh của các nghiên cứu đầu tiên của Châu Âu về Phương Đông định vị phương Đông là phi lý và lạc hậu đối lập với phương Tây hợp lý và tiến bộ. Xác định phương Đông như một tầm nhìn tiêu cực của chính nó, vì nó kém hơn, không chỉ làm tăng ý thức về bản thân của West West, mà còn là một cách ra lệnh cho phương Đông và khiến phương Tây biết đến nó để có thể bị chi phối và kiểm soát. Do đó, diễn ngôn của chủ nghĩa phương Đông đóng vai trò là sự biện minh về ý thức hệ của chủ nghĩa đế quốc phương Tây thời kỳ đầu, vì nó hình thành một cơ thể tri thức và ý tưởng hợp lý hóa sự kiểm soát xã hội, văn hóa, chính trị và kinh tế của các lãnh thổ khác.</td>\n",
              "      <td>652</td>\n",
              "      <td>phi lý và lạc hậu</td>\n",
              "      <td>3</td>\n",
              "      <td>{'answer_start': [652], 'text': ['phi lý và lạc hậu']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57308f6b8ab72b1400f9c581</td>\n",
              "      <td>Các văn bản phương Tây đầu tiên đề cập đến phương Đông mô tả người dân là gì?</td>\n",
              "      <td>Chủ nghĩa phương Đông, theo lý thuyết của Edward Said, đề cập đến cách phương Tây phát triển một địa lý tưởng tượng của phương Đông. Địa lý tưởng tượng này dựa trên một diễn ngôn thiết yếu đại diện cho cả sự đa dạng cũng như thực tế xã hội của phương Đông. Thay vào đó, bằng cách thiết yếu phương Đông, bài diễn văn này sử dụng ý tưởng về bản sắc dựa trên địa điểm để tạo ra sự khác biệt và khoảng cách giữa \"chúng ta\" phương Tây và \"họ\" phương Đông, hay \"ở đây\" ở phương Tây và \"ở đó\" ở phương Đông. Sự khác biệt này đặc biệt rõ ràng trong các tác phẩm văn bản và hình ảnh của các nghiên cứu đầu tiên của Châu Âu về Phương Đông định vị phương Đông là phi lý và lạc hậu đối lập với phương Tây hợp lý và tiến bộ. Xác định phương Đông như một tầm nhìn tiêu cực của chính nó, vì nó kém hơn, không chỉ làm tăng ý thức về bản thân của West West, mà còn là một cách ra lệnh cho phương Đông và khiến phương Tây biết đến nó để có thể bị chi phối và kiểm soát. Do đó, diễn ngôn của chủ nghĩa phương Đông đóng vai trò là sự biện minh về ý thức hệ của chủ nghĩa đế quốc phương Tây thời kỳ đầu, vì nó hình thành một cơ thể tri thức và ý tưởng hợp lý hóa sự kiểm soát xã hội, văn hóa, chính trị và kinh tế của các lãnh thổ khác.</td>\n",
              "      <td>652</td>\n",
              "      <td>phi lý và lạc hậu</td>\n",
              "      <td>3</td>\n",
              "      <td>{'answer_start': [652], 'text': ['phi lý và lạc hậu']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  \\\n",
              "0  5726e179dd62a815002e93b1   \n",
              "1  5727580bf1498d1400e8f69e   \n",
              "2  572665ff708984140094c4c3   \n",
              "3  57308f6b8ab72b1400f9c581   \n",
              "4  57308f6b8ab72b1400f9c581   \n",
              "\n",
              "                                                                                 question  \\\n",
              "0                Những dược sĩ nào có khả năng tìm kiếm giáo dục bổ sung sau trường dược?   \n",
              "1       Cách đánh vần nào của Genghis phù hợp nhất với cách phát âm có thể xảy ra của nó?   \n",
              "2  Nguyên tắc nào liên quan đến sự hình thành các lỗi và tuổi của các chuỗi mà chúng cắt?   \n",
              "3           Các văn bản phương Tây đầu tiên đề cập đến phương Đông mô tả người dân là gì?   \n",
              "4           Các văn bản phương Tây đầu tiên đề cập đến phương Đông mô tả người dân là gì?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           context  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Do sự phức tạp của thuốc bao gồm chỉ định cụ thể, hiệu quả của chế độ điều trị, an toàn của thuốc (nghĩa là tương tác thuốc) và các vấn đề tuân thủ của bệnh nhân (trong bệnh viện và tại nhà), nhiều dược sĩ thực hành tại bệnh viện được giáo dục và đào tạo nhiều hơn sau khi học trường dược một cư dân hành nghề dược và đôi khi theo sau là một cư dân khác trong một khu vực cụ thể. Những dược sĩ đó thường được gọi là dược sĩ lâm sàng và họ thường chuyên về các ngành dược khác nhau. Ví dụ, có những dược sĩ chuyên về huyết học / ung thư, HIV / AIDS, bệnh truyền nhiễm, chăm sóc quan trọng, thuốc cấp cứu, độc dược, dược học hạt nhân, quản lý đau, tâm thần, phòng chống đông máu, thảo dược, quản lý thần kinh / động kinh, nhi khoa , dược sĩ sơ sinh và nhiều hơn nữa.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Một giả thuyết cho rằng cái tên này bắt nguồn từ một phiên bản được khai hóa của từ tiếng Mông Cổ và tiếng Thổ Nhĩ Kỳ, có nghĩa là \"đại dương\", \"đại dương\" hoặc \"trải rộng\". (Hồ Baikal và đại dương được người Mông Cổ gọi là tenggis. Tuy nhiên, dường như nếu họ có ý định gọi Genghis tenggis thì họ có thể đã nói, và viết, \"Tenggis Khan\", mà họ không biết.) Zhèng (tiếng Trung:) có nghĩa là \"đúng\", \"chỉ\" hoặc \"đúng\", sẽ nhận được từ bổ nghĩa tính từ Mông Cổ -s, tạo ra \"Jenggis\", trong tiếng La Mã thời trung cổ sẽ được viết là \"Genghis\". Có khả năng cách phát âm tiếng Mông Cổ của thế kỷ 13 sẽ rất phù hợp với \"Chính tả\".   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Nguyên tắc mối quan hệ xuyên suốt liên quan đến sự hình thành các lỗi và tuổi của các chuỗi mà qua đó chúng cắt. Lỗi là trẻ hơn những tảng đá họ cắt; theo đó, nếu một lỗi được tìm thấy thâm nhập vào một số hình dạng chứ không phải các lỗi trên nó, thì các hình dạng bị cắt cũ hơn lỗi và các phần không bị cắt phải nhỏ hơn lỗi. Tìm giường chính trong những tình huống này có thể giúp xác định xem lỗi là lỗi thông thường hay lỗi lực đẩy.   \n",
              "3  Chủ nghĩa phương Đông, theo lý thuyết của Edward Said, đề cập đến cách phương Tây phát triển một địa lý tưởng tượng của phương Đông. Địa lý tưởng tượng này dựa trên một diễn ngôn thiết yếu đại diện cho cả sự đa dạng cũng như thực tế xã hội của phương Đông. Thay vào đó, bằng cách thiết yếu phương Đông, bài diễn văn này sử dụng ý tưởng về bản sắc dựa trên địa điểm để tạo ra sự khác biệt và khoảng cách giữa \"chúng ta\" phương Tây và \"họ\" phương Đông, hay \"ở đây\" ở phương Tây và \"ở đó\" ở phương Đông. Sự khác biệt này đặc biệt rõ ràng trong các tác phẩm văn bản và hình ảnh của các nghiên cứu đầu tiên của Châu Âu về Phương Đông định vị phương Đông là phi lý và lạc hậu đối lập với phương Tây hợp lý và tiến bộ. Xác định phương Đông như một tầm nhìn tiêu cực của chính nó, vì nó kém hơn, không chỉ làm tăng ý thức về bản thân của West West, mà còn là một cách ra lệnh cho phương Đông và khiến phương Tây biết đến nó để có thể bị chi phối và kiểm soát. Do đó, diễn ngôn của chủ nghĩa phương Đông đóng vai trò là sự biện minh về ý thức hệ của chủ nghĩa đế quốc phương Tây thời kỳ đầu, vì nó hình thành một cơ thể tri thức và ý tưởng hợp lý hóa sự kiểm soát xã hội, văn hóa, chính trị và kinh tế của các lãnh thổ khác.   \n",
              "4  Chủ nghĩa phương Đông, theo lý thuyết của Edward Said, đề cập đến cách phương Tây phát triển một địa lý tưởng tượng của phương Đông. Địa lý tưởng tượng này dựa trên một diễn ngôn thiết yếu đại diện cho cả sự đa dạng cũng như thực tế xã hội của phương Đông. Thay vào đó, bằng cách thiết yếu phương Đông, bài diễn văn này sử dụng ý tưởng về bản sắc dựa trên địa điểm để tạo ra sự khác biệt và khoảng cách giữa \"chúng ta\" phương Tây và \"họ\" phương Đông, hay \"ở đây\" ở phương Tây và \"ở đó\" ở phương Đông. Sự khác biệt này đặc biệt rõ ràng trong các tác phẩm văn bản và hình ảnh của các nghiên cứu đầu tiên của Châu Âu về Phương Đông định vị phương Đông là phi lý và lạc hậu đối lập với phương Tây hợp lý và tiến bộ. Xác định phương Đông như một tầm nhìn tiêu cực của chính nó, vì nó kém hơn, không chỉ làm tăng ý thức về bản thân của West West, mà còn là một cách ra lệnh cho phương Đông và khiến phương Tây biết đến nó để có thể bị chi phối và kiểm soát. Do đó, diễn ngôn của chủ nghĩa phương Đông đóng vai trò là sự biện minh về ý thức hệ của chủ nghĩa đế quốc phương Tây thời kỳ đầu, vì nó hình thành một cơ thể tri thức và ý tưởng hợp lý hóa sự kiểm soát xã hội, văn hóa, chính trị và kinh tế của các lãnh thổ khác.   \n",
              "\n",
              "   answer_start                    text  c_id  \\\n",
              "0           416        dược sĩ lâm sàng     0   \n",
              "1           469                \"Jenggis     1   \n",
              "2            11  mối quan hệ xuyên suốt     2   \n",
              "3           652       phi lý và lạc hậu     3   \n",
              "4           652       phi lý và lạc hậu     3   \n",
              "\n",
              "                                                      answers  \n",
              "0       {'answer_start': [416], 'text': ['dược sĩ lâm sàng']}  \n",
              "1               {'answer_start': [469], 'text': ['\"Jenggis']}  \n",
              "2  {'answer_start': [11], 'text': ['mối quan hệ xuyên suốt']}  \n",
              "3      {'answer_start': [652], 'text': ['phi lý và lạc hậu']}  \n",
              "4      {'answer_start': [652], 'text': ['phi lý và lạc hậu']}  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IIK5Iy_OoDwJ"
      },
      "outputs": [],
      "source": [
        "tds = Dataset.from_pandas(train)\n",
        "vds = Dataset.from_pandas(dev)\n",
        "\n",
        "\n",
        "squad = DatasetDict()\n",
        "\n",
        "squad['train'] = tds\n",
        "squad['validation'] = vds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F7WoQnEoDwK",
        "outputId": "998837f5-8165-4dc6-d429-f3225d91ac6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['index', 'question', 'context', 'answer_start', 'text', 'c_id', 'answers'],\n",
              "        num_rows: 87187\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['index', 'question', 'context', 'answer_start', 'text', 'c_id', 'answers'],\n",
              "        num_rows: 34575\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx-aVAWVoDwK",
        "outputId": "35a2721f-5139-478e-d2ab-f91bd7432e4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 686/686 [00:00<00:00, 687kB/s]\n",
            "Downloading: 100%|██████████| 450M/450M [00:27<00:00, 17.4MB/s]   \n",
            "Some weights of the model checkpoint at subhasisj/vi-TAPT-MLM-MiniLM were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at subhasisj/vi-TAPT-MLM-MiniLM and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "pretraining_language = language\n",
        "# model = AutoModelForQuestionAnswering.from_pretrained(\"microsoft/Multilingual-MiniLM-L12-H384\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(f\"subhasisj/{pretraining_language}-TAPT-MLM-MiniLM\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LyTyDfJUoDwL"
      },
      "outputs": [],
      "source": [
        "pad_on_right = tokenizer.padding_side == \"right\"\n",
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
        "def prepare_train_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "def prepare_validation_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"index\"][sample_index])\n",
        "\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sy-2gbptoDwM"
      },
      "outputs": [],
      "source": [
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d73229e2a02c4bdf9facf1a35b10a504",
            "de03c60d2ad84e5a82faee6c200745ab",
            "d9e3fe8ac0eb492098779ea1393c3ddc",
            "c6a052905d964080b6a3e25aeef1b79a",
            "c640b73583ab4506bd2ce4f32b897a36",
            "8c14c1d17dff44e299d548bc9aef316f",
            "f5dd67e66a2041049eff6411dc582c71",
            "ff459669cec846b2a4061e1cec69c9ae",
            "a82f4abed3ad4cc0bf22d9bc73b1487e",
            "0343f3bb7e064b29aca8c5d732f6716a",
            "82fe1dc137d04ea4ab6604f10d96996f"
          ]
        },
        "id": "K_rmHgEsoDwM",
        "outputId": "7737a283-c97d-4e75-8a0a-63aa7b139916"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 88/88 [00:41<00:00,  2.11ba/s]\n",
            "100%|██████████| 35/35 [00:18<00:00,  1.89ba/s]\n"
          ]
        }
      ],
      "source": [
        "squad = squad.map(prepare_train_features,batched=True,remove_columns=squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b59e4bba9f42442f80f886c9a729ac24",
            "6414c6e99d68473e9494cc9a2987c55a",
            "9924247934634c03a98742a7a7423d2d",
            "ff9e0283a15342f2b51e7bdec7946fc9",
            "891f35ffba484c81980e37136e3fce1b",
            "c2a8d46316e9424d8910358cd64afbc3",
            "c21dcadefa6f42f0b87c4c454206c258",
            "df95b749de404bfc85838ae3452f0147",
            "53b138d36e344832892e0d970014c7f7",
            "3ef7b9d2afd640f3984197043bc918b2",
            "960830d55acb4ad5bc52fa37a3a20251"
          ]
        },
        "id": "KEEJXUIKsHm1",
        "outputId": "45a9a477-876c-4f07-9e11-5b748f89db56"
      },
      "outputs": [],
      "source": [
        "# squad[\"validation\"] = squad[\"validation\"].map(prepare_validation_features,batched=True,remove_columns=squad[\"validation\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Dbe50ff6oDwN"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "args = TrainingArguments(\n",
        "    f\"./{pretraining_language}-finetuned-squad-qa-minilmv2-{batch_size}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate = 3e-5,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size = batch_size,\n",
        "    warmup_ratio = 0.1,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    num_train_epochs = 5,\n",
        "    fp16=True,\n",
        "    weight_decay = 0.01,\n",
        "    push_to_hub=True,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8qlIMf3-oDwN"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8ThBRtloDwN",
        "outputId": "6ec5e5a7-a593-437b-eed3-0d7288894da6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8 into local empty directory.\n",
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=squad[\"train\"],\n",
        "    eval_dataset=squad[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "LtnGrNAMoDwO",
        "outputId": "32e467a3-19d7-4844-cbb9-6977e884c978"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 91135\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 7120\n",
            "  7%|▋         | 500/7120 [11:10<2:22:36,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9579, 'learning_rate': 2.106741573033708e-05, 'epoch': 0.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 1000/7120 [21:48<2:09:24,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.1669, 'learning_rate': 2.8661048689138576e-05, 'epoch': 0.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1424/7120 [30:45<2:00:40,  1.27s/it]***** Running Evaluation *****\n",
            "  Num examples = 36529\n",
            "  Batch size = 8\n",
            "                                                     \n",
            " 20%|██        | 1424/7120 [34:24<2:00:40,  1.27s/it]Saving model checkpoint to ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-1424\n",
            "Configuration saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-1424\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.4979444742202759, 'eval_runtime': 218.9565, 'eval_samples_per_second': 166.832, 'eval_steps_per_second': 20.858, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-1424\\pytorch_model.bin\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-1424\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-1424\\special_tokens_map.json\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\special_tokens_map.json\n",
            "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n",
            " 21%|██        | 1500/7120 [36:40<2:00:05,  1.28s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.5664, 'learning_rate': 2.6320224719101124e-05, 'epoch': 1.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 2000/7120 [47:15<1:48:20,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.3263, 'learning_rate': 2.3979400749063673e-05, 'epoch': 1.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 2500/7120 [57:50<1:37:45,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.2377, 'learning_rate': 2.1638576779026218e-05, 'epoch': 1.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2848/7120 [1:05:12<1:30:00,  1.26s/it]***** Running Evaluation *****\n",
            "  Num examples = 36529\n",
            "  Batch size = 8\n",
            "                                                       \n",
            " 40%|████      | 2848/7120 [1:08:51<1:30:00,  1.26s/it]Saving model checkpoint to ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-2848\n",
            "Configuration saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-2848\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.3258743286132812, 'eval_runtime': 218.8901, 'eval_samples_per_second': 166.883, 'eval_steps_per_second': 20.864, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-2848\\pytorch_model.bin\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-2848\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-2848\\special_tokens_map.json\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\special_tokens_map.json\n",
            " 42%|████▏     | 3000/7120 [1:12:59<1:27:08,  1.27s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1677, 'learning_rate': 1.9297752808988763e-05, 'epoch': 2.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 3500/7120 [1:23:33<1:16:30,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0856, 'learning_rate': 1.695692883895131e-05, 'epoch': 2.46}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 4000/7120 [1:34:08<1:05:46,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0536, 'learning_rate': 1.461610486891386e-05, 'epoch': 2.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 4272/7120 [1:39:52<1:00:10,  1.27s/it]***** Running Evaluation *****\n",
            "  Num examples = 36529\n",
            "  Batch size = 8\n",
            "                                                       \n",
            " 60%|██████    | 4272/7120 [1:43:43<1:00:10,  1.27s/it]Saving model checkpoint to ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-4272\n",
            "Configuration saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-4272\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.3132898807525635, 'eval_runtime': 230.2918, 'eval_samples_per_second': 158.621, 'eval_steps_per_second': 19.831, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-4272\\pytorch_model.bin\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-4272\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-4272\\special_tokens_map.json\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\special_tokens_map.json\n",
            "Deleting older checkpoint [vi-finetuned-squad-qa-minilmv2-8\\checkpoint-1424] due to args.save_total_limit\n",
            " 63%|██████▎   | 4500/7120 [1:49:42<57:08,  1.31s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0078, 'learning_rate': 1.2275280898876405e-05, 'epoch': 3.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 5000/7120 [2:00:54<47:18,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9533, 'learning_rate': 9.934456928838951e-06, 'epoch': 3.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 5500/7120 [2:11:58<34:40,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9568, 'learning_rate': 7.598314606741573e-06, 'epoch': 3.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 5696/7120 [2:16:11<30:22,  1.28s/it]***** Running Evaluation *****\n",
            "  Num examples = 36529\n",
            "  Batch size = 8\n",
            "                                                     \n",
            " 80%|████████  | 5696/7120 [2:19:55<30:22,  1.28s/it]Saving model checkpoint to ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-5696\n",
            "Configuration saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-5696\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.3103300333023071, 'eval_runtime': 223.4314, 'eval_samples_per_second': 163.491, 'eval_steps_per_second': 20.44, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-5696\\pytorch_model.bin\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-5696\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-5696\\special_tokens_map.json\n",
            "Deleting older checkpoint [vi-finetuned-squad-qa-minilmv2-8\\checkpoint-2848] due to args.save_total_limit\n",
            " 84%|████████▍ | 6000/7120 [2:26:59<24:27,  1.31s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9161, 'learning_rate': 5.25749063670412e-06, 'epoch': 4.21}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████▏| 6500/7120 [2:38:29<14:18,  1.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.891, 'learning_rate': 2.916666666666667e-06, 'epoch': 4.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 7000/7120 [2:49:34<02:37,  1.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8859, 'learning_rate': 5.758426966292135e-07, 'epoch': 4.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7120/7120 [2:52:11<00:00,  1.31s/it]***** Running Evaluation *****\n",
            "  Num examples = 36529\n",
            "  Batch size = 8\n",
            "                                                     \n",
            "100%|██████████| 7120/7120 [2:55:54<00:00,  1.31s/it]Saving model checkpoint to ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-7120\n",
            "Configuration saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-7120\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.3335455656051636, 'eval_runtime': 222.8103, 'eval_samples_per_second': 163.947, 'eval_steps_per_second': 20.497, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-7120\\pytorch_model.bin\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-7120\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-7120\\special_tokens_map.json\n",
            "Deleting older checkpoint [vi-finetuned-squad-qa-minilmv2-8\\checkpoint-4272] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./vi-finetuned-squad-qa-minilmv2-8\\checkpoint-5696 (score: 1.3103300333023071).\n",
            "100%|██████████| 7120/7120 [2:56:17<00:00,  1.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 10577.9662, 'train_samples_per_second': 43.078, 'train_steps_per_second': 0.673, 'train_loss': 1.3618155436569386, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7120, training_loss=1.3618155436569386, metrics={'train_runtime': 10577.9662, 'train_samples_per_second': 43.078, 'train_steps_per_second': 0.673, 'train_loss': 1.3618155436569386, 'epoch': 5.0})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./vi-finetuned-squad-qa-minilmv2-8\n",
            "Configuration saved in ./vi-finetuned-squad-qa-minilmv2-8\\config.json\n",
            "Model weights saved in ./vi-finetuned-squad-qa-minilmv2-8\\pytorch_model.bin\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\special_tokens_map.json\n",
            "Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n",
            "Upload file pytorch_model.bin: 100%|█████████▉| 448M/448M [1:34:10<00:00, 103kB/s]  remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "remote: error: cannot lock ref 'refs/heads/main': is at c5355ca2a0f4fe545c53597728eaa44150d96c48 but expected c83ce5183b7a9fa525301ab7426d1943808587b2        \n",
            "To https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8\n",
            " ! [remote rejected] main -> main (failed to update ref)\n",
            "error: failed to push some refs to 'https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8'\n",
            "\n",
            "Upload file pytorch_model.bin: 100%|██████████| 448M/448M [1:34:22<00:00, 83.0kB/s]\n",
            "Upload file runs/May13_17-00-42_DESKTOP-638AK67/events.out.tfevents.1652441467.DESKTOP-638AK67: 100%|██████████| 7.17k/7.17k [1:34:22<?, ?B/s]\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "remote: Enforcing permissions...        \nremote: Allowed refs: all        \nremote: error: cannot lock ref 'refs/heads/main': is at c5355ca2a0f4fe545c53597728eaa44150d96c48 but expected c83ce5183b7a9fa525301ab7426d1943808587b2        \nTo https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8\n ! [remote rejected] main -> main (failed to update ref)\nerror: failed to push some refs to 'https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\huggingface_hub\\repository.py:1018\u001b[0m, in \u001b[0;36mRepository.git_push\u001b[1;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1016'>1017</a>\u001b[0m             \u001b[39mif\u001b[39;00m return_code:\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1017'>1018</a>\u001b[0m                 \u001b[39mraise\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError(\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1018'>1019</a>\u001b[0m                     return_code, process\u001b[39m.\u001b[39margs, output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1019'>1020</a>\u001b[0m                 )\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1021'>1022</a>\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n",
            "\u001b[1;31mCalledProcessError\u001b[0m: Command '['git', 'push', '--set-upstream', 'origin', 'main']' returned non-zero exit status 1.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32md:\\Repositories\\masters-research-qa\\pretraining\\TAPT\\03. finetuning-oodomain-labelled.ipynb Cell 21'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repositories/masters-research-qa/pretraining/TAPT/03.%20finetuning-oodomain-labelled.ipynb#ch0000034?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39mTraining Completed\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\transformers\\trainer.py:2875\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[1;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2871'>2872</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_world_process_zero():\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2872'>2873</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2874'>2875</a>\u001b[0m git_head_commit_url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepo\u001b[39m.\u001b[39;49mpush_to_hub(\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2875'>2876</a>\u001b[0m     commit_message\u001b[39m=\u001b[39;49mcommit_message, blocking\u001b[39m=\u001b[39;49mblocking, auto_lfs_prune\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2876'>2877</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2877'>2878</a>\u001b[0m \u001b[39m# push separately the model card to be independant from the rest of the model\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/transformers/trainer.py?line=2878'>2879</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\huggingface_hub\\repository.py:1251\u001b[0m, in \u001b[0;36mRepository.push_to_hub\u001b[1;34m(self, commit_message, blocking, clean_ok, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1248'>1249</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgit_add(auto_lfs_track\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1249'>1250</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgit_commit(commit_message)\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1250'>1251</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgit_push(\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1251'>1252</a>\u001b[0m     upstream\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morigin \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_branch\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1252'>1253</a>\u001b[0m     blocking\u001b[39m=\u001b[39;49mblocking,\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1253'>1254</a>\u001b[0m     auto_lfs_prune\u001b[39m=\u001b[39;49mauto_lfs_prune,\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1254'>1255</a>\u001b[0m )\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\all-purpose-gpu\\lib\\site-packages\\huggingface_hub\\repository.py:1023\u001b[0m, in \u001b[0;36mRepository.git_push\u001b[1;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1017'>1018</a>\u001b[0m                 \u001b[39mraise\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError(\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1018'>1019</a>\u001b[0m                     return_code, process\u001b[39m.\u001b[39margs, output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1019'>1020</a>\u001b[0m                 )\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1021'>1022</a>\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1022'>1023</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(exc\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1024'>1025</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m blocking:\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/envs/all-purpose-gpu/lib/site-packages/huggingface_hub/repository.py?line=1026'>1027</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mstatus_method\u001b[39m():\n",
            "\u001b[1;31mOSError\u001b[0m: remote: Enforcing permissions...        \nremote: Allowed refs: all        \nremote: error: cannot lock ref 'refs/heads/main': is at c5355ca2a0f4fe545c53597728eaa44150d96c48 but expected c83ce5183b7a9fa525301ab7426d1943808587b2        \nTo https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8\n ! [remote rejected] main -> main (failed to update ref)\nerror: failed to push some refs to 'https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8'\n"
          ]
        }
      ],
      "source": [
        "trainer.push_to_hub(\"Training Completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./vi-finetuned-squad-qa-minilmv2-8/model\n",
            "Configuration saved in ./vi-finetuned-squad-qa-minilmv2-8/model\\config.json\n",
            "Model weights saved in ./vi-finetuned-squad-qa-minilmv2-8/model\\pytorch_model.bin\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8/model\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8/model\\special_tokens_map.json\n",
            "Saving model checkpoint to ./vi-finetuned-squad-qa-minilmv2-8\n",
            "Configuration saved in ./vi-finetuned-squad-qa-minilmv2-8\\config.json\n",
            "Model weights saved in ./vi-finetuned-squad-qa-minilmv2-8\\pytorch_model.bin\n",
            "tokenizer config file saved in ./vi-finetuned-squad-qa-minilmv2-8\\tokenizer_config.json\n",
            "Special tokens file saved in ./vi-finetuned-squad-qa-minilmv2-8\\special_tokens_map.json\n",
            "Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8\n",
            "   c5355ca..95f8d9b  main -> main\n",
            "\n",
            "Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/subhasisj/vi-finetuned-squad-qa-minilmv2-8\n",
            "   95f8d9b..47bd3c8  main -> main\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model(\"./vi-finetuned-squad-qa-minilmv2-8/model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !tar -zcvf vi-finetuned-squad-qa-minilmv2-32.tar.gz /kaggle/working/vi-finetuned-squad-qa-minilmv2-32/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.chdir(r'/kaggle/working')\n",
        "# from IPython.display import FileLink\n",
        " \n",
        "# FileLink(r'./vi-finetuned-squad-qa-minilmv2-32.tar.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u9xbeOgoDwO"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(\"Training Completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rCmwUA2oDwO"
      },
      "outputs": [],
      "source": [
        "validation_features = squad[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9NzrkpioDwO"
      },
      "outputs": [],
      "source": [
        "# validation_features = squad_valid.map(\n",
        "#     prepare_validation_features,\n",
        "#     batched=True,\n",
        "#     remove_columns=squad_valid.column_names\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXIgFsSRoDwP"
      },
      "outputs": [],
      "source": [
        "raw_predictions = trainer.predict(validation_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCryGMeKoDwP"
      },
      "outputs": [],
      "source": [
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8y6KQSfoDwP"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "examples = squad_valid\n",
        "features = validation_features\n",
        "\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "features_per_example = collections.defaultdict(list)\n",
        "for i, feature in enumerate(features):\n",
        "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoWxjPKaoDwP"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                        or offset_mapping[start_index] == []\n",
        "                        or offset_mapping[end_index] == []\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
        "        # if not squad_v2:\n",
        "            # predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "        # else:\n",
        "        answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
        "        predictions[example[\"id\"]] = answer\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn0mipzqoDwQ"
      },
      "outputs": [],
      "source": [
        "final_predictions = postprocess_qa_predictions(squad_valid, validation_features, raw_predictions.predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMNy31HyoDwQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERR0PihNoDwQ"
      },
      "outputs": [],
      "source": [
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in squad_valid]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "03. finetuning-oodomain-labelled.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "988aea983bc49b4de6555a9151e99301026502c3eb4adf0dc68b340a087613cc"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('all-purpose-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "020a97d66b384582a0d0e6fbc82d909e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0343f3bb7e064b29aca8c5d732f6716a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "084772ca72054d339b2437fe17ba88ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15f6efce0301452899e29953d79f51ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_37302452a84048ae9fb47ab74952188e",
            "placeholder": "​",
            "style": "IPY_MODEL_084772ca72054d339b2437fe17ba88ba",
            "value": ""
          }
        },
        "232b82c2311b488e891e5a47490a986a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_56c490a66b2f47d59489974e905ef1f2",
            "style": "IPY_MODEL_66e8b8093a054f9bbe32d8b9b22637d9",
            "tooltip": ""
          }
        },
        "37302452a84048ae9fb47ab74952188e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38502d84bf4d42d5a9d0cf6b3e647f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3a4b4338218b481eb63eeed22d73c809": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ef7b9d2afd640f3984197043bc918b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51fe991ba33d4b7eaa83bf4a86c04a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53b138d36e344832892e0d970014c7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56c490a66b2f47d59489974e905ef1f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6414c6e99d68473e9494cc9a2987c55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2a8d46316e9424d8910358cd64afbc3",
            "placeholder": "​",
            "style": "IPY_MODEL_c21dcadefa6f42f0b87c4c454206c258",
            "value": "100%"
          }
        },
        "66e8b8093a054f9bbe32d8b9b22637d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "82fe1dc137d04ea4ab6604f10d96996f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "891f35ffba484c81980e37136e3fce1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c14c1d17dff44e299d548bc9aef316f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d7e8ce3bc15458bae47fba843b3b75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d865c31a5b5c46debf61dbd1a3d92227",
            "placeholder": "​",
            "style": "IPY_MODEL_51fe991ba33d4b7eaa83bf4a86c04a47",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "960830d55acb4ad5bc52fa37a3a20251": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9924247934634c03a98742a7a7423d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df95b749de404bfc85838ae3452f0147",
            "max": 35,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53b138d36e344832892e0d970014c7f7",
            "value": 35
          }
        },
        "a82f4abed3ad4cc0bf22d9bc73b1487e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b59e4bba9f42442f80f886c9a729ac24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6414c6e99d68473e9494cc9a2987c55a",
              "IPY_MODEL_9924247934634c03a98742a7a7423d2d",
              "IPY_MODEL_ff9e0283a15342f2b51e7bdec7946fc9"
            ],
            "layout": "IPY_MODEL_891f35ffba484c81980e37136e3fce1b"
          }
        },
        "ba936bb9738a49d0b2b89f6e7724525a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d7e8ce3bc15458bae47fba843b3b75b",
              "IPY_MODEL_15f6efce0301452899e29953d79f51ab",
              "IPY_MODEL_232b82c2311b488e891e5a47490a986a",
              "IPY_MODEL_d203ced429e540ad9325d8887fccfbaf",
              "IPY_MODEL_cee7e824a6864589bc9d58e0bb1a2397"
            ],
            "layout": "IPY_MODEL_cbac3350f96a4fdebb45b71a9bec1510"
          }
        },
        "bc8403ee9ec24b8c90334b1e58dfccd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c21dcadefa6f42f0b87c4c454206c258": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2a8d46316e9424d8910358cd64afbc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c640b73583ab4506bd2ce4f32b897a36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a052905d964080b6a3e25aeef1b79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0343f3bb7e064b29aca8c5d732f6716a",
            "placeholder": "​",
            "style": "IPY_MODEL_82fe1dc137d04ea4ab6604f10d96996f",
            "value": " 87/87 [01:22&lt;00:00,  1.13ba/s]"
          }
        },
        "cbac3350f96a4fdebb45b71a9bec1510": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "cee7e824a6864589bc9d58e0bb1a2397": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Use password",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3a4b4338218b481eb63eeed22d73c809",
            "style": "IPY_MODEL_38502d84bf4d42d5a9d0cf6b3e647f54",
            "tooltip": ""
          }
        },
        "d203ced429e540ad9325d8887fccfbaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_020a97d66b384582a0d0e6fbc82d909e",
            "placeholder": "​",
            "style": "IPY_MODEL_bc8403ee9ec24b8c90334b1e58dfccd5",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. <br> <i>Logging in with your username and password is deprecated and\nwon't be possible anymore in the near future. You can still use them for now by\nclicking below.</i> </center>"
          }
        },
        "d73229e2a02c4bdf9facf1a35b10a504": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de03c60d2ad84e5a82faee6c200745ab",
              "IPY_MODEL_d9e3fe8ac0eb492098779ea1393c3ddc",
              "IPY_MODEL_c6a052905d964080b6a3e25aeef1b79a"
            ],
            "layout": "IPY_MODEL_c640b73583ab4506bd2ce4f32b897a36"
          }
        },
        "d865c31a5b5c46debf61dbd1a3d92227": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9e3fe8ac0eb492098779ea1393c3ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff459669cec846b2a4061e1cec69c9ae",
            "max": 87,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a82f4abed3ad4cc0bf22d9bc73b1487e",
            "value": 87
          }
        },
        "de03c60d2ad84e5a82faee6c200745ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c14c1d17dff44e299d548bc9aef316f",
            "placeholder": "​",
            "style": "IPY_MODEL_f5dd67e66a2041049eff6411dc582c71",
            "value": "100%"
          }
        },
        "df95b749de404bfc85838ae3452f0147": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5dd67e66a2041049eff6411dc582c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff459669cec846b2a4061e1cec69c9ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9e0283a15342f2b51e7bdec7946fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ef7b9d2afd640f3984197043bc918b2",
            "placeholder": "​",
            "style": "IPY_MODEL_960830d55acb4ad5bc52fa37a3a20251",
            "value": " 35/35 [04:52&lt;00:00,  6.84s/ba]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
